{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model of efficientnet_b1\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torchvision.models import efficientnet_b1\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    # \"Implement label smoothing.\"\n",
    "    def __init__(self, size, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        x = x.log()\n",
    "        true_dist = x.data.clone()  \n",
    "        true_dist.fill_(self.smoothing / (self.size - 1))  \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "\n",
    "        self.true_dist = true_dist\n",
    "        print(x.shape, true_dist.shape)\n",
    "\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = 'C:\\\\Users\\\\User\\\\Desktop\\\\tiffany\\\\ML_final\\\\data'\n",
    "datasets_dir = ''\n",
    "\n",
    "batch_size = 16  \n",
    "gpu = '0'  \n",
    "num_workers = 12  \n",
    "seed = 135  \n",
    "note = 'cm_2'  \n",
    "amp = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### exp setting\n",
    "nb_epoch = 28 \n",
    "lr_begin = (batch_size / 256) * 0.1  # learning rate at begining\n",
    "use_amp = 2  # use amp to accelerate training\n",
    "\n",
    "\n",
    "##### data settings\n",
    "data_dir = join('data', datasets_dir)\n",
    "data_sets = ['train', 'train'] ##test->train\n",
    "nb_class = len(\n",
    "    os.listdir(join(data_dir, data_sets[0]))\n",
    ")  # get number of class via img folders automatically\n",
    "exp_dir = 'result/{}{}'.format(datasets_dir, note)  # the folder to save model\n",
    "\n",
    "\n",
    "##### CUDA device setting\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "\n",
    "##### Random seed setting\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dataloader setting\n",
    "re_size = 512\n",
    "crop_size = 448\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((re_size, re_size)),\n",
    "        transforms.RandomCrop(crop_size, padding=8),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((re_size, re_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = ImageFolder(root=join(data_dir, data_sets[0]), transform=train_transform)\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Model settings\n",
    "\n",
    "net = efficientnet_b1(pretrained=True)\n",
    "\n",
    "in_features = net.classifier[1].in_features\n",
    "\n",
    "net.classifier[1] = nn.Linear(in_features, nb_class)\n",
    "\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### optimizer setting\n",
    "LSLoss = LabelSmoothingLoss(\n",
    "    classes=nb_class, smoothing=0.08\n",
    ")  # label smoothing to improve performance\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), lr=lr_begin, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copyfile('110700039_training.ipynb', os.path.join(exp_dir, 'train.ipynb'))\n",
    "\n",
    "with open(os.path.join(exp_dir, 'train_log.csv'), 'w+') as file:\n",
    "    file.write('Epoch, lr, Train_Loss, Train_Acc, Test_Acc\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Apex\n",
    "if use_amp == 1:  # use nvidia apex.amp\n",
    "    print('\\n===== Using NVIDIA AMP =====')\n",
    "    from apex import amp\n",
    "\n",
    "    net.cuda()\n",
    "    net, optimizer = amp.initialize(net, optimizer, opt_level='O1')\n",
    "    with open(os.path.join(exp_dir, 'train_log.csv'), 'a+') as file:\n",
    "        file.write('===== Using NVIDIA AMP =====\\n')\n",
    "elif use_amp == 2:  # use torch.cuda.amp\n",
    "    print('\\n===== Using Torch AMP =====')\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    with open(os.path.join(exp_dir, 'train_log.csv'), 'a+') as file:\n",
    "        file.write('===== Using Torch AMP =====\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "##### 2 - Training #####\n",
    "########################\n",
    "net.cuda()\n",
    "min_train_loss = float('inf')\n",
    "max_eval_acc = 0\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print('\\n===== Epoch: {} ====='.format(epoch))\n",
    "    net.train()  # set model to train mode, enable Batch Normalization and Dropout\n",
    "    lr_now = optimizer.param_groups[0]['lr']\n",
    "    train_loss = train_correct = train_total = idx = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, ncols=80)):\n",
    "        idx = batch_idx\n",
    "\n",
    "        if inputs.shape[0] < batch_size:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()  # Sets the gradients to zero\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        ##### amp setting\n",
    "        if use_amp == 1:  # use nvidia apex.amp\n",
    "            x = net(inputs)\n",
    "            loss = LSLoss(x, targets)\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "        elif use_amp == 2:  # use torch.cuda.amp\n",
    "            with autocast():\n",
    "                x = net(inputs)\n",
    "                loss = LSLoss(x, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            x = net(inputs)\n",
    "            loss = LSLoss(x, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(x.data, 1)\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += predicted.eq(targets.data).cpu().sum()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = 100.0 * float(train_correct) / train_total\n",
    "    train_loss = train_loss / (idx + 1)\n",
    "    print(\n",
    "        'Train | lr: {:.4f} | Loss: {:.4f} | Acc: {:.3f}% ({}/{})'.format(\n",
    "            lr_now, train_loss, train_acc, train_correct, train_total\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ##### Evaluating model with test data every epoch\n",
    "    with torch.no_grad():\n",
    "        net.eval()  # set model to eval mode, disable Batch Normalization and Dropout\n",
    "        eval_set = ImageFolder(\n",
    "            root=join(data_dir, data_sets[-1]), transform=test_transform\n",
    "        )\n",
    "        eval_loader = DataLoader(\n",
    "            eval_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "        eval_correct = eval_total = 0\n",
    "        for _, (inputs, targets) in enumerate(tqdm(eval_loader, ncols=80)):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            x = net(inputs)\n",
    "            _, predicted = torch.max(x.data, 1)\n",
    "            eval_total += targets.size(0)\n",
    "            eval_correct += predicted.eq(targets.data).cpu().sum()\n",
    "        eval_acc = 100.0 * float(eval_correct) / eval_total\n",
    "        print(\n",
    "            '{} | Acc: {:.3f}% ({}/{})'.format(\n",
    "                data_sets[-1], eval_acc, eval_correct, eval_total\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ##### Logging\n",
    "        with open(os.path.join(exp_dir, 'train_log.csv'), 'a+') as file:\n",
    "            file.write(\n",
    "                '{}, {:.4f}, {:.4f}, {:.3f}%, {:.3f}%\\n'.format(\n",
    "                    epoch, lr_now, train_loss, train_acc, eval_acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "        ##### save model with highest acc\n",
    "        if eval_acc > max_eval_acc:\n",
    "            max_eval_acc = eval_acc\n",
    "            torch.save(\n",
    "                net.state_dict(),\n",
    "                os.path.join(exp_dir, 'max_acc_cm2.pth'),\n",
    "                _use_new_zipfile_serialization=False,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
